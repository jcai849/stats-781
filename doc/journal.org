[2019-07-14 Sun]: GOTO [[#program-dependencies][Program Dependencies]] section, continue through Preparation, Insight, and Visualisation
* Table of Contents                                                     :TOC:
- [[#colophon][COLOPHON]]
- [[#initial-survey][Initial Survey]]
  - [[#prescribed-reading][Prescribed Reading]]
  - [[#initial-consideration-of-application-features][Initial Consideration of Application Features]]
  - [[#twitter][Twitter]]
  - [[#subtitles][Subtitles]]
  - [[#r-packages][R Packages]]
  - [[#other-text-analytics-applications][Other Text Analytics Applications]]
  - [[#scope-determination][Scope Determination]]
- [[#initial-feature-considerations][Initial Feature Considerations]]
  - [[#introduction][Introduction]]
  - [[#processing][Processing]]
  - [[#within-text-analytics][Within-Text Analytics]]
  - [[#between-text-analytics][Between-Text Analytics]]
  - [[#stopwords][Stopwords]]
  - [[#visualisation][Visualisation]]
  - [[#text-summarisation][Text Summarisation]]
  - [[#search-function][Search Function]]
  - [[#topic-modelling][Topic Modelling]]
  - [[#sentiment-distribution][Sentiment Distribution]]
  - [[#conditional-analytics][Conditional Analytics]]
  - [[#wrapper-functions][Wrapper Functions]]
  - [[#visualisation-1][Visualisation]]
  - [[#ggpage][ggpage]]
- [[#initial-data-types-survey][Initial Data Types Survey]]
  - [[#test-corpus][Test Corpus]]
  - [[#free-response-data][Free-Response Data]]
  - [[#data-types-for-implementation][Data types for implementation]]
- [[#initial-considerations-of-program-structure][Initial Considerations of Program Structure]]
  -  [[#2019-06-13-thu-notes][[2019-06-13 Thu] Notes]]
  - [[#2019-07-10-wed-notes][[2019-07-10 Wed] Notes]]
  - [[#note-on-non-destructive-editing][Note on Non-Destructive Editing]]
- [[#program-dependencies][Program Dependencies]]
  - [[#helper-functions][Helper Functions]]
- [[#program-layer-preparation][Program Layer: Preparation]]
  - [[#importing][Importing]]
  - [[#formatting][Formatting]]
  - [[#filtering][Filtering]]
  - [[#lemmatisation][Lemmatisation]]
  - [[#stemming][Stemming]]
  - [[#stopwords-1][Stopwords]]
  - [[#object-preparation][Object Preparation]]
  - [[#sectioning][Sectioning]]
  - [[#grouping][Grouping]]
- [[#program-layer-insight][Program Layer: Insight]]
  - [[#word-insight][Word Insight]]
  - [[#aggregate-insight][Aggregate Insight]]
  - [[#wrapper][Wrapper]]
- [[#program-layer-visualisation][Program Layer: Visualisation]]
  - [[#distribution-14][Distribution]]
  - [[#rank][Rank]]
  - [[#score][Score]]
  - [[#structure][Structure]]
  - [[#relation][Relation]]
  - [[#wrapper-1][Wrapper]]
- [[#testing--demonstration][Testing / Demonstration]]
  - [[#source][Source]]
  - [[#ungrouped][Ungrouped]]
  - [[#grouped][Grouped]]
  - [[#ggpage-1][ggpage]]
- [[#shiny-frontend][Shiny Frontend]]

* COLOPHON
Work was all done in emacs, with the code in R, tangled through org mode
* Initial Survey
** Prescribed Reading
- [[https://www.tidytextmining.com][Text Mining with R]]: [[./text_mining_with_r.org][notes]]
- [[https://m-clark.github.io/text-analysis-with-R/][Text Analysis with R]]
** Initial Consideration of Application Features
*UX/UI* must be intuitive, modelling the inherently non-linear workflow,
while fitting in with the iNZight / lite environment. I wrote some notes
on UX/UI [[./ux_ui.org][here]].

*Text Classification* seems to be a hot topic in sentiment analysis, but
the question remains of whether it is within our scope (I suspect not).
If it were, [[https://cran.r-project.org/web/packages/openNLP/][openNLP]], along with some pre-made [[https://datacube.wu.ac.at/src/contrib/][models]], would likely serve 
this topic well. An interesting example of text classification in the
Dewey Decimal System is given [[http://creatingdata.us/models/SRP-classifiers][here]].

*[[../reading/Thurner2015%20-%20Understanding%20Zipfs%20Law%20of%20Word%20Frequencies%20through%20Sample%20Space%20Collapse%20in%20Sentence%20Formation.pdf][Zipf's Law]]* is referred to regularly in text analysis. Should we
demonstrate this directly, as part of some other analysis, or not at
all?

The *form of analysis* will vary enormously with different forms of
text. There are some things constant for all forms of text, but a good
deal is very specific. For example, the information of interest will
differ between novels, discourse (interviews, plays, scripts), twitter
posts, survey response data, or others. It may be worthwhile to either
focus solely on one, or give the option to specify the type of text.

*Data structures* will change based on the text type, and the packages
used. With a tidy dataframe as our base structure, it is easy enough to
convert to specific objects required by various packages, harder to
convert back.

There is a natural link between *text analysis and Linguistics*, and a
significant amount of the terminology in the field reflects that. Our
application requires far more market share than one that a mention in a
third year linguistics paper provides, and so linguistics is not going
to be our primary focus. Regardless, many forms of analysis require some
linguistic theory, such as those dependent on Part of Speech tagging, so
it is still worthwhile to keep in mind
** Twitter
Twitter data looks particularly interesting, as it is a constantly
updating, rich source of information. I wrote up some notes on text
mining twitter [[./text_mining_twitter.org][here]]. It would be particularly interesting to view
twitter data in the context of discourse analysis.
** Subtitles
Subtitles are a unique form of text that would be very interesting to
analyse. Subtitles for films and TV Series can be obtained easily from
the site [[https://www.opensubtitles.org/en/search/subs][opensubtitles]], though
obtaining subtitles programatically may be more difficult. It clearly is
possible, as VLC has an inbuilt feature, as does [[https://github.com/zerratar/SubSync][subsync]], which is written in
C#, so would require a port to R (probably not worth it for us at this
point). Subtitles usually come as .srt files, and once the file is
obtained, it's easy enough to import and work with it in R with the
package [[https://github.com/fkeck/subtools][subtools]].
** R Packages
[[https://quanteda.io/articles/pkgdown/comparison.html][Here]] is a useful comparison between the major text mining packages. CRAN also has
a [[https://cran.r-project.org/web/views/NaturalLanguageProcessing.html][task view]] specifically for Natural Language Processing, offering many
packages relevant to this project. Interestingly, they are split by
linguistic category; Syntax, Semantics, and Pragmatics. The further from
syntax the package is, the far more interesting it intuitively appears
(eg. word count vs sentiment analysis). Some packages of interest
include:

- [[https://github.com/juliasilge/tidytext][tidytext]] :: is a text-mining
  package using tidy principles, providing excellent interactivity with
  the tidyverse, as documented in the book
  [[https://www.tidytextmining.com][Text Mining with R]]
- [[http://tm.r-forge.r-project.org/][tm]] :: is a text-mining framework
  that was the go-to for text mining in R, but appears to have been made
  redundant by tidytext and quanteda of late
- [[https://quanteda.io/][quanteda]] :: sits alone next to qdap in the
  Pragmatics section of the NLP task view, and offers a similar
  capability to tidytext, though from a more object-oriented paradigm,
  revolving around /corpus/ objects. It also has extensions such as
  offering readability scores, something that may be worth implementing.
- [[https://trinker.github.io/qdap/vignettes/qdap_vignette.html][qdap]] ::  is a "quantitative discourse analysis package", an extremely rich set
  of tools for the analysis of discourse in text, such as may arise from
  plays, scripts, interviews etc. Includes output on length of discourse
  for agents, turn-taking, and sentiment within passages of speech. This
  looks to me like the most insight that could be gained from a text.
- [[https://github.com/trinker/sentimentr][sentimentr]] :: is a rich
  sentiment analysis and tokenising package, with features including
  dealing with negation, amplification, etc. in multi-sentence level
  analysis. An interesting feature is the ability to output text with
  sentences highlighted according to their inferred sentiment
- [[https://rstudio.github.io/dygraphs/][dygraphs]] :: is a time-series
  visualisation package capable of outputting very clear interactive
  time-series graphics, useful for any time-series in the text analysis
  module
- [[https://github.com/thomasp85/gganimate][gganimate]] :: produces  animations on top of the [[https://github.com/tidyverse/ggplot2][ggplot]] package, offering
  powerful insights. [[https://www.r-bloggers.com/investigating-words-distribution-with-r-zipfs-law-2/][Here]] is an example demonstrating Zipf's Law
- [[https://github.com/bnosac/textrank][textrank]] :: has the unique idea
  of extracting keywords automatically from a text using the pagerank
  algorithm (pagerank studied in depth in STATS 320) - my exploration of
  the package is documented [[./textrank_exploration.Rmd][here]]
- Packages for obtaining text:

  - [[https://cran.r-project.org/web/packages/gutenbergr/index.html][gutenbergr]] :: from Project Gutenberg
  - [[https://rtweet.info/][rtweet]] :: from Twitter
  - [[https://cran.r-project.org/web/packages/WikipediaR/index.html][wikipediar]] :: from Wikipedia

- [[https://github.com/EmilHvitfeldt/ggpage][ggpage]] :: produces impressive page-view charts with features such as
     word highlighting, allowing for a clear overview of a text and
     it's structure, with probable use in our search feature function

- [[https://github.com/thomasp85/gganimate][gganimate]] :: produces animated charts, which can be useful if
     additional, regular, and low /n/ dimensions exist in the data

--------------

Additionally, there are some packages that may not necessarily be useful
for the end user, but may help for our development needs. These
include:
- [[https://github.com/bnosac/udpipe][udpipe]] performs
tokenisation, parts of speech tagging (which serves as the foundation
for textrank), and more, based on the well-recognised C++
[[http://ufal.mff.cuni.cz/udpipe][udpipe library]], using the [[https://universaldependencies.org][Universal Treebank]]
- [[https://github.com/bnosac/BTM][BTM]] performs Biterm Topic Modelling,
which is useful for "finding topics in short texts (as occurs in short
survey answers or twitter data)". It uses a somewhat complex sampling
procedure, and like LDA topic modelling, requires a corpus for
comparison. Based on [[https://github.com/xiaohuiyan/BTM][C++ BTM]] 
- [[https://github.com/bnosac/crfsuite][crfsuite]] provides a modelling
framework, which is currently outside our current scope, but could be
useful later 
- In the analysis / removal of names, an important component of a text,
[[https://github.com/ironholds/humaniformat/][humaniformat]] is likely to be useful
- [[https://cran.r-project.org/web/views/WebTechnologies.html][CRAN Task View: Web Technologies and Services]] for importing texts from the
internet

** Other Text Analytics Applications
The field of text analytics applications is rather diverse, with most
being general analytics applications with text analytics as a feature of
the application. Some of the applications (general and specific) are
given:

- [[http://www.bnosac.be/index.php/products/txtminer][txtminer]] is a
  web app for analysing text at a deep level (with something of a
  linguistic focus) over multiple languages, for an "educated citizen
  researcher"
** Scope Determination
The scope of the project is naturally limited by the amount of time
available to do it. As such, exploration of topics such as discourse
analysis, while interesting, is beyond the scope of the project.
Analysis of text must be limited to regular texts, and comparisons
between them. The application must give the greatest amount of insight
to a regular user, in the shortest amount of time, into what the text is
actually about.

[[http://usresp-student.shinyapps.io/text_analysis][Cassidy's project]] was intended to create this, and I have written
notes on it [[./cassidy_notes.org][here]].

Ultimately, I am not completely sold on the idea that term frequencies
and other base-level statistics really give that clear a picture of what
a text is about. It can give some direction, and it can allow for broad
classification of works (eg. a novel will usually have character names
at the highest frequency ranks, scientific works usually have domain
specific terms), but I think word frequencies are less useful to the
analyst than to the algorithms they feed into, such as tf-idf, that may
be more useful. As such, I don't think valuable screen space should be
taken up by low-level statistics such as term frequencies. To me, the
situation is somewhat akin to [[https://en.wikipedia.org/wiki/Anscombe's_quartet][Anscombe's Quartet]], where the base 
statistics leave a good deal of information out, term frequencies being 
analogous to the modal values.

Additionally, sentiment is really just one part of determining the
semantics of a text. I think too much focus is put on sentiment, which
in practice is something of a "happiness meter". I would like to include
other measurement schemes, such as readability, formality, etc.

Some kind of context in relation to the universal set of texts would be
ideal as well, I think a lot of this analysis occurs in a vacuum, and
insights are hard to come by - something like Google n-grams would be
ideal.

I'm picturing a single page, where the analyst can take one look and
have a fair idea of what a text is about. In reality it will have to be
more complex than that, but that is my lead at the moment. With this in
mind, I want to see keywords, more on /structure/ of a text, context,
and clear, punchy graphics showing not /just/ sentiment, but several
other key measurements.

* Initial Feature Considerations
** Introduction
The application essentially consists of a feature-space, with the area
being divided in three; [[*Processing][Processing]], [[*Within-Text Analytics][Within-Text Analytics]], and
[[*Between-Text Analytics][Between-Text Analytics]]. This follows the general format of much of
what is capable in text analysis, and what is of interest to us and our
end users. The UI will likely reflect this, dividing into seperate
windows/panes/tabs to accomodate. Let's look at them in turn:
** Processing
In order for text to be analysed, it must be imported and processed. A
lot of this is an iterative process, coming back for further processing
after analysis etc. Importing will have a "type" selection ability for
the user, where they can choose from a small curated list of easy-access
types, such as gutenberg search, twitter, etc. The option for a custom
text-type is essential, allowing .txt, and for the particularly advanced
end-user, .csv.

Once the file is imported/type is downloaded, the option should exist to
allow the specification of divisions in the text. In a literary work,
these include "chapter", "part", "canto", etc. A twitter type would
allow division by author, by tweet, etc. An important aspect of this
processing is to have a clear picture of what the data should look like.
Division of a text should be associated with some visualisation of the
resulting structure of the text, such as a horizontal bar graph showing
the raw count of text (word count) for each division - this would allow
immediate insight into the correctness of the division, by sighting
obvious errors immediately, and allowing fine tuning so that, for
example, the known number of chapters match up with the number of
divisions. We could implement a few basic division operators in regex,
while following the philosophy of allowing custom input if wanted.
Example regex for "Chapter" could be
=/[Cc]hapter[.:]?[   ]{0,10}-?[  ]{0,10}([0-9]|[ivxIVX]*))/g=, something
the end user is likely not wanting to input themselves.

Removal and transformation is another important processing step for
text, with stopwords and lemmatisation being invaluable. The option
should exist to remove specific types of words, which can again come
from prespecified lists. An aspect worth considering is if this should
be done in a table manipulation, or a model - or both, with the length
of the text deciding automatically based on sensible defaults. Again,
the need for a clear picture of the data is essential, with some visual
indication of the data during transformation and removal essential; this
could take the form of some basic statistics, such as a ranking of terms
by frequencies, and some random passage chosen.

Processing multiple documents is also essential. The importation is
something that has to be got right, otherwise it'll be more complex than
it already is, and the end-user will lose interest before the show even
begins. My initial thoughts are of a tabbed import process, with each
tab holding the processing tasks for each individual document, however
this won't scale well to large corpus imports.

** Within-Text Analytics
Within-text analytics should have options to look at the whole text as
it is, whether to look by division, or whether to look at the entire
imported corpus as a whole.

A killer feature here is the production of a summary; a few key
sentences that summarise the text. It's a case of using text to describe
text, but done effectively, it has the potential to compress a large
amount of information into a small, human-understandable object.

Related to the summary, keywords in the text will give a good indication
of topics and tone of the text, as well as perhaps more grammatical
notions, such as authorial word choices. There is the possibility of
using keywords as a basis for other features, such as the ability to use
a search engine to find related texts from the keywords.

Bigrams and associated terms are also excellent indicators of a text.
Something I particularly liked in Cassidy's project was the ability to
search for a term, and see what was related to it. In that case, the
text was "Peter Pan", and searching for a character's name yielded a
wealth of information of the emotions and events attached to the
character.

Sentiment is a feature that has been heavily developed by the field of
text analytics, seeing a broad variety of uses. here, it would be worth
examining sentiment, by word and over the length of the text overall.

** Between-Text Analytics
As in within-text analytics, between-text analytics should have
options for specifying the component of the text that is of interest;
here, the two major categories would be comparisons between divisions
within an individual text, and comparisons between full texts.

Topic modelling gives an idea of what some topics are between texts -
something odd to me is that there isn't a huge amount of information on
topic modelling purely within a text, it always seems to be between
texts (LDA etc.)

tf-idf for a general overview of terms more or less unique to different
texts.

Summarisation between all texts would also be enormously useful.

** Stopwords
After noting that stopword removal impacted important n-grams when a
stopword made up some component of the n-gram, it becomes very
worthwhile to not only include an active capacity to view what current
stopwords exist, but also to have alternative lists of stopwords. The
following summarises some research into stopwords and common practices
around them;

- StackOverflow removes the top 10,000 most common english words in
  "related queries" for their SQL search engine
  (https://stackoverflow.blog/2008/12/04/podcast-32/)
- The [[https://github.com/quanteda/stopwords][stopwords]] =R= package includes several lists of stopwords. Among
  these, of note are:
  - [[http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop][SMART]]: The stopword lists based on the SMART (System for the
    Mechanical Analysis and Retrieval of Text)Information Retrieval
    System, an information retrieval system developed at Cornell
    University inthe 1960s.
  - [[http://snowball.tartarus.org/algorithms/english/stop.txt][snowball]]: It is a small string processing language designed for
    creating stemming algorithms for use in Information Retrieval.
  - [[https://github.com/stopwords-iso/stopwords-iso/blob/master/stopwords-iso.json][iso]]: The most comprehensive stopwords for any language

The package we are using extensively, tidytext, has both SMART and
snowball lists, as well as [[http://www.lextek.com/manuals/onix/stopwords1.html][onix]], which bills itself as " probably the
most widely used stopword list. It covers a wide number of stopwords
without getting too aggressive and including too many words which a
user might search upon." Of note is that all of the lists are included
in one dataframe, so it should be filtered before being used, unlike
how we have been using it. snowball is clearly the shortest, and I
think may be worth having as the default, with SMART (the most
extensive) and onix as secondary options. We are not in the role of
providing a computationally efficient search engine, only removing
words that contribute little but noise.

In terms of implementation within our program, we ought to have the
ability to add custom stopwords. In keeping with the philosophy of
having our data clearly visible, this will necessitate a "temporary
stopwords" list. In the process of implementation, we will have to
make assesments of whether it will run too slowly if allowed to
influence charts and output in real timme, so manual refreshes would
be required. Additionally, it will be good to have a running set of
statistics keeping available what has been done to the data (including
more than just stopword removal)
** Visualisation
With so much of the conceptual space of text analytic visualisation
being taken up with far from optimal charts, there is a need to
experiment with alternative visualisations; We explore some [[file:sent-vis.org][here]]
** Text Summarisation
[[https://en.wikipedia.org/wiki/Automatic_summarization][Wikipedia: Automatic Summarisation]]

Text summarisation creates enormous insight, especially from a long
text. There are a variety of different techniques, of varying
effectiveness and efficiency. A famous example of automatic text
summarisation comes from [[https://www.reddit.com/user/autotldr][autotoldr]], a bot on reddit that automatically
generates summaries of news articles in 4-5 sentences. Autotldr is
powered by [[https://smmry.com/about][SMMRY]], which explains it's algorithm as working through the
following steps:

1. Associate words with their grammatical counterparts. (e.g "city"
   and "cities")
2. Calculate the occurrence of each word in the text.
3. Assign each word with points depending on their popularity.
4. Detect which periods represent the end of a sentence. (e.g "Mr."
   does not).
5. Split up the text into individual sentences.
6. Rank sentences by the sum of their words' points.
7. Return X of the most highly ranked sentences in chronological
   order.

The two main approaches to automatic summarisation are extractive and
abstractive; *Extractive* uses some subset of the original text to
form a summary, while *abstractive* techniques form semantic
representations of the text. Here, we will stick to the clearer,
simpler, extractive techniques for now.

[[https://github.com/bnosac/textrank][textrank]] has the unique idea of extracting keywords automatically from
a text using the pagerank algorithm (pagerank studied in depth in
STATS 320) - my exploration of the package is documented [[./textrank_exploration.Rmd][here]]. At
present, the R implementation of it creates errors for large text
files, but it is worth exploring more into it - whether it is the
implementation, or if it is the algorithm itself.

Hvidfeldt is a prolific blogger focussing on text analysis - he put up
this tutorial on incorporating textrank with tidy methods: [[https://www.hvitfeldt.me/blog/tidy-text-summarization-using-textrank/][tidy
textRank]]

Further summarisation experimentation is continued [[file:summarisation_experimentation.Rmd][here]]

After further testing, I have found LexRank to work significantly
faster, while generating similar results, thus being favourable for
summarisation. It appears that Textrank wins in the ability to
generate keywords, and does so extremely quickly. Despite the speed
gain in using LexRank for summarisation, it still takes several
seconds on my i5 dual-core, to run, however this is offset by the
verbosity of the function assuring me that it isn't hanging.

LexRank and textRank appear to exist complimentarily to one another.
Below is a brief summary of how they work

*** TextRank

TextRank essentially finds the most representative sentence of a text
based on some similarity measure to other sentence.

By dividing a text into sentences, measures of similarity between every
sentence is calculated (by any number of possible similarity measures),
producing an adjacency matrix of a graph with nodes being sentences,
edge weights being similarity. The PageRank algorithm is then run on
this graph, deriving the best connected sentences, and thereby the most
representative sentences. A list is produced giving sentences with their
corresponding PageRank. The top $n$ sentences can be chosen, then output
in chronological order, to produce a summary.

In the generation of keywords, the same process described is typically
run on unigrams, with the similarity measure being co-occurance.

*** LexRank
    :PROPERTIES:
    :CUSTOM_ID: lexrank
    :END:

LexRank is essentially the same as textRank, however uses
[[https://en.wikipedia.org/wiki/Cosine_similarity][cosine similarity]] of tf-idf vectors as it's measure of similarity. LexRank is better at
working across multiple texts, due to the inclusion of a heuristic known
as "Cross-Sentence Information Subsumption (CSIS)"
** Search Function
The analyst is not expected to be entirely familiar with the texts
under analysis; this is partly the purpose of this program. Hence,
there are likely to be terms, keywords, and relationships that the
program reveals, and are a surprise to the analyst, and context is
necessary to understand them. A search function has been identified as
useful in meeting this problem, where a word is entered in search, and
contextual passages are returned. Useful in the results would be
indications of location of each passage in the greater text, as well
as if multiple texts are present, the name of the text it belongs to.

** Topic Modelling
Topic Modelling appears to serve a useful purpose in text analytics,
with LDA being the primary implementation, requiring multiple texts,
and a Document-Term Matrix. My exploration with topic modelling is
located [[file:topic-modelling.Rmd][here]]. It could be worth investigating other forms of topic
modelling, especially within-text.

[2019-05-17 Fri] I checked other forms - their complexity requires a
great deal of time to understand if I want to implement them
intelligently; better to stick with LDA, which, while also complex, is
well used enough to be considered standard.
** Sentiment Distribution
Over a large /n/ dataset such as free-response surveys, it may be
useful to calculate the sentiment for each response, and consider the
statistical properties of the distribution of sentiments. [[file:sent-dist.org][Here]] is an
exploration of free-response data forming a sentiment distribution.
** Conditional Analytics
The idea of conditional analytics is of interest to me, especially for
high /n/ datasets such as large free-response surveys. Particularly, I
want to know, given some condition, how does the subset behave? For
example, [[file:sent-dist.org][given]] a negative sentiment, what is the most representative
response? Or, given that some common word, what is the distribution of
sentiment
** Wrapper Functions
In order to begin implementation, I have defined wrapper functions for
the primary features. The intention is to create a higher layer of
abstraction for the features as well as ease of use. I begin with the
text summarisation feature; the details are below
*** Text Summarisation
Link to [[file:~/curr/stats-781/src/summ-wrapper.R][src]]
Link to [[file:~/curr/stats-781/test/summ-wrapper-test.R][test]]
Arguments:
- x = input dataframe with column titled "word"
- n = n-many sentences
- style = style of output (chart, dataframe, text)
- dim = dimension of chart
- engine = textrank/lexrank
- type = sentences/keywords etc.

Working through, I have come to come realisation that a complete
wrapper function may not necessarily be ideal; rather, a pipeline may
be better - this is because a wrapper function, with, e.g., a plotting
function at the outermost layer, would require a full recalculation of
the inner functions for every parameter change in the plot - what may
be better is the creation of a pipeline that leaves most functions as
they are but just creates more suitable objects to pass as arguments
to the functions. This is something of a "memory cheap; processing
expensive" principle. The display wrapper functions would then be
taking complete objects only 

<2019-05-22 Wed> Chris clarified the role of wrappers here being more
of a "layer" level, layers being:
- word/n-gram;
  - Word frequency
  - Bigram frequency
  - pairwise word correlations
  - textrank keywords
- sentence;
  - textrank
  - lexrank
- topic level
- sentiment level
** Visualisation
 Visualisation of text is proving to be a more complex area than I
 first assumed. Prior to this project, the only visualisation I knew of
 was word clouds, which I have come to understand to be about as
 useless as an unlabelled pie chart.

 Text visualisation is essentially the attempt to efficiently relay
 insights gained from text analytics. In the
 preparation-insight-visualisation layers, it is the final layer.
 Visualisation is not limited to just charts; for our purposes, a well
 crafted and formatted table may be just as good at conveying
 information.

 The form of the insight determines the form of the visualisation. So
 far, insights all give a "score". Thus, the visualisation, showing a
 mapping between a text (categorical) and a numerical insight
 (numberical) varaiable, can only take a few forms, ideally showing the
 relative scores and ranking of specific text items, or a distribution
 of the entire set.

 At base, nearly everything is neatly categorical-numeric, able to be
 represented by bars/lollipops.

 Pairwise correlation is slightly different, being a numerical function
 of two categorical arguments; best represented in either a searchable
 table, or a correlation matrix

 Getting more advanced, for small data, ggpage type visualisations will
 be excellent for sentiment and word/bigram frequency, as well as
 ranking keywords.

 Finally, when grouping is implemented, colouring or facetting by group
 will be what makes this analysis package better than any competitors.

 [2019-07-01 Mon] After implementing grouping, the issue with arranging
 bars in a barplot by rank within each group is that ggplot arranges
 bars through the ordering of factor levels. The problem is that each
 instance of a word in every group shares the same level ordering, so
 while a word may rank highly overall, but less than others in a
 particular group, it will retain the high ordering overall in the
 facet for that group, leading to inaccuracies.

** ggpage
ggpage is an extension to ggplot to allow the rendering of text in a
page-like representation as a manipulable image. 
Example
#+begin_src R :results value :colnames yes :hline yes :session rsession1 :tangle no :comments link :exports both :eval never-export
library(tidyverse)
library(ggpage)
head(tinderbox)
#+end_src

#+RESULTS
# A tibble: 6 x 2
  text                                                              book        
  <chr>                                                             <chr>       
1 "A soldier came marching along the high road: \"Left, right - le… The tinder-…
2 had his knapsack on his back, and a sword at his side; he had be… The tinder-…
3 and was now returning home. As he walked on, he met a very frigh… The tinder-…
4 witch in the road. Her under-lip hung quite down on her breast, … The tinder-…
5 "and said, \"Good evening, soldier; you have a very fine sword, … The tinder-…
6 knapsack, and you are a real soldier; so you shall have as much … The tinder-…

ggpage can make immediate plots, but using =ggpage_build= and
=ggpage_plot=, complex functions can be formed in the immediate
representation from build before plotting. The representation takes
the following form:
#+begin_src R :results output :colnames yes :hline yes :session rsession1 :tangle no :comments link :exports both :eval never-export
tinderbox %>%
  ggpage_build() 
#+end_src

#+RESULTS:
# A tibble: 2,908 x 9
   word     book            page  line  xmin  xmax  ymin  ymax index_line
   <chr>    <chr>          <int> <int> <dbl> <dbl> <dbl> <dbl> <chr>     
 1 a        The tinder-box     1     1    91    90  -114  -117 1-1       
 2 soldier  The tinder-box     1     1    99    92  -114  -117 1-1       
 3 came     The tinder-box     1     1   104   100  -114  -117 1-1       
 4 marching The tinder-box     1     1   113   105  -114  -117 1-1       
 5 along    The tinder-box     1     1   119   114  -114  -117 1-1       
 6 the      The tinder-box     1     1   123   120  -114  -117 1-1       
 7 high     The tinder-box     1     1   128   124  -114  -117 1-1       
 8 road     The tinder-box     1     1   133   129  -114  -117 1-1       
 9 left     The tinder-box     1     1   138   134  -114  -117 1-1       
10 right    The tinder-box     1     1   144   139  -114  -117 1-1       
# … with 2,898 more rows

This is set up solely for novels, and there is no way yet to implement
grouping (as at ggpage v0.2.2.9000), but this may be useful. ggpage
requires the scoring to be defined within the ggpage_build dataframe
form - we can make use of this if we apply the insight functions to
it. Entirely coincidentally, we have used precisely the same naming
conventions for the input dataframe to ggpage_build (column named
'text'), and the insight functions inside ggpage_build (working on
column named "word"). Some tests are given in the test file. The
primary issue with using ggpage is that the insight is applied as a
/part/ of the visualisation, rather than being seperate to it, as with
all the others.
* Initial Data Types Survey
The application requires the capacity to smoothly work with diverse
data types. For this to occur, a test corpus must be developed, and
some important data types picked out.
** Test Corpus
It is essential to test on a broad variety of texts in order to create
the most general base application, so a "test set" will have to be
developed. All data is stored in the folder [[file:c:/Users/User/Desktop/stats-781/data][data]]

*Must have*

- Literature (eg. Dante's Divine Comedy)
- Survey response data (eg. nzqhs, Cancer Society)
- Transcript; lack of punctuation may cause difficulties in processing
  sentences.
- Twitter

*Would be nice*

- article
  - journal (scientific, social)
  - news
  - blog
  - wikipedia
- discourse
  - interview
  - subtitles
- documentation
  - product manual
  - technical user guide

** Free-Response Data
Free Response Data (as in survey forms etc.) has been identified as an
area of high potential for the application. Two datasets have been
used to run typical text analyses upon, with the exploration [[file:free-response.Rmd][here]].
Upon close inspection, there are subtleties worth exploring [[file:further-free-response.org][further]]
especially in bigrams and keywords.
** Data types for implementation
In the production of wrapper functions, we require data types that
work well with all functions that are required. For the purpose of
word-level summarisation, the following features require functions with the
associated data types as arguments:
- Word frequency: =tidytext::unnest_tokens=
  - @param tbl: A data frame
- Bigram frequency: =tidytext::unnest_tokens=
  - @param tbl: A data frame
- pairwise word correlations: =widyr::pairwise_cor=
  - @param tbl: Table
  - @param: item: Item to compare; will end up in ‘item1’ and ‘item2’
    columns
  - @param feature: Column describing the feature that links one item
    to others
- textrank keywords: =textrank::textrank_keywords=
  - @param x: a character vector of words.

Thinking even earlier in the pipeline, the processing section requires
functions to remove stopwords- this requires =tidytext::unnest_tokens=
again, meaning a dataframe. The issue is that if we operate on groups,
then we require a function that takes a vector as argument. Perhaps
more thought is needed in understanding what grouped operations should
look like in text analytics. Alternatively, we could create a function
that takes a dataframe as input, with the option to name groups to
perform group operations upon. 

Another issue that arises is the elimination of sentences and
structure upon the unnesting of tokens. What may be worthwhile is to
create a dataframe such as the following:

#+ATTR_LATEX: :booktabs :align l | l | l | l | l  :float t
| grouping vars | ... | doc_id | paragraph_id | sentence_id | word_id | word |
|---------------+-----+--------+--------------+-------------+---------+------|
|               |     |        |              |             |         |      |

In which case, we should start at the very beginning, looking at text
import wrapper functions, enabling them to output a dataframe of this
type such that the remaining process is entirely predictable.

Current files for wrappers:
[[file:~/curr/stats-781/src/prep-for-insight.R][prep-for-insight.R]]

Note: [2019-06-10 Mon]: determined that line number is more general
than paragraph: paragraph can be inferred from line number.

As @ [2019-06-13 Thu], I have found the dataframe form as described
prior to be extremely valuable. The implementation of all wrappers
should have as the aim to preserve the structure as much as possible,
only adding additional columns to the dataframe resulting from the
function.
*** Text Analytics wrappers
 [2019-05-29 Wed]: Chris approved the datatype. Work will begin on the
 wrappers, using this datatype. He raised the very valid point on how
 pairwise corelations between words should possibly use groups as
 their similarity component, rather than sentences. e.g., correlation
 of words between survey responses. *note: groups are always nested,
 and conditioning is actually filtering*

Important to note: Different punctuation marks exist, and despite some
visual similarities, are not recognised as equivalent on the computer:
for example, "’" and "'" are different. Selecting "alice's" as a
stopword will not filter out "alice’s". While on the topic, it may be
worthwhile to incorporate regex ability for the application. CLI
integration would be a dream, but not so useful for school and
undergraduate students.

* Initial Considerations of Program Structure
**  [2019-06-13 Thu] Notes
  - Read [[http://r-pkgs.org][r-pkgs.org]]. Notes: A working prototype will be built before
    formally packaging it; this is to allow for greater flexibility and
    experimentation without worry about breaking the package structure.
    All the source code for functions are located in the src folder,
    grouped according to their functional category.
  - Further intentions: a rigorous, clean implementation of grouping and
    conditioning (generalised as filtering) is something I believe to be
    important to make this package stand out from the crowd. Upon the
    function set all working, I think this would be worth pursuing. The
    structure of the internal datatype has been kept specifically so
    that grouping and filtering are efficient, lossless, and simple
    operations.
Dataframe form:
#+ATTR_LATEX: :booktabs :align l | l | l | l | l  :float t
| grouping vars | ... | doc_id | paragraph_id | sentence_id | word_id | word |
|---------------+-----+--------+--------------+-------------+---------+------|
|               |     |        |              |             |         |      |
** [2019-07-10 Wed] Notes
I have done some further thinking today, especially
following the meeting yesterday; destructive edits to the text are a
serious problem to the integrity of the text, where all insight
actions require starting from scratch as soon as any different types
of input are needed. An example stems from experimenting with ggpage and
realising that when stopwords are removed, the structure of the text
is heavily hollowed out. After some thought, my solution is the following;

*Processing*: Start with the importation and formatting of text,
 keeping every single word and it's identification, down to the
 capitalisation. Further options include (for example) lemmatisation,
 and stopwords. In keeping with the spirit of non-destructive edits,
 each add a column: lemmatisation adds a lemmatised form of row's
 word, and stopwords adds a boolean value regarding the status of the
 lemma. A final processing function creates a new row for the insight to be
 performed on, based on the processing options (to use lemmas,
 stopwords etc.). Groups are then declared.

*Insights* looks for the insight column, and adds some output column
 based upon it. The only changes I will have to make to the existing
 functions will be to look for the insight column. A potential
 difficulty is that they will have to be capable of dealing with
 missing values (now that stopwords are just removed with NA in place)

*Visualisations* will be exactly the same. A new, neat bit will be
 that ggpage is simply a case of =ggpage_build= of the original import
 and a =cbind=, then =ggpage_plot(aes(fill = insight))=.

(End of Solution) In addition, I have been thinking about UI. Shiny
apps often have a paged, scrolling structure like a webpage, but I
think text analytics may require a different format, due to the
continual return to the processing stage, as well as the large amount
of processing required for many operations, thus leading to slow,
laggy pages. I think the "SAS format" may be a winning formula, where
tickboxes, radios, and inputs on one high level page are tweaked, then
a button is pressed to produce the output. This would lend itself
really well to going back and tweaking, as well as the feature of code
generation. It obeys the KISS principle, which wins it points in my
book. 

Preparation is now divided into importation, grouping, formatting,
then processing. In detail:
- Import :: bringing in text from various formats, convert to simple table
- Group :: section text by groups, for which later operations will be uniquely performed on
- Format :: format the text into a standard object that can be operated on
- Process :: remove stopwords, lemmatise, filter, other lossy transformations
** Note on Non-Destructive Editing
 Destructive editing is the practice where the original input can't be
 attained after the transformation. It is non-Injective, and
 non-invertible. Thus, when certain changes are required, an earlier
 state is needed. Tidytext has made the decision to encourage
 destructive edits, which is acceptable when the user is a programmer
 with full control over every possible variable assignment, but not for
 a GUI user. Hence, we have made the explicit decision to have
 non-destructive transformations only, after hitting repeated
 roadblocks related to Destructive edits. Memory is cheap for
 computers, and summarisation functions can always be delayed, to
 retain as much information, as many degrees of freedom as possible.
 The concept of nondestructive edits is not new; graphic design relies
 upon it, with an example given for photoshop at the [[https://helpx.adobe.com/photoshop/using/nondestructive-editing.html][Adobe Website]]
* Program Dependencies
#+begin_src R :session rsession1 :tangle ~/stats-781/src/depends.R
  library(tidyverse)
  library(readr)
  library(readxl)
  library(tidytext)
  library(textrank)
  library(lexRankr)
  library(ggpage)
  library(textstem)
  library(glue)
  library(tcltk)
#+end_src
I am considering using [[https://davisvaughan.github.io/furrr/][furrr]] for parallel or distributed processing
performance enhancements, though I want to get all functionality
implemented first before performing that kind of optimisation.
** Helper Functions
*** Unrestricted if-expression
Having conditionals as expressions rather than statements grants the
ability for direct assignment of the evaluation. Base =ifelse= and
tidyverse =if_else= impose the restriction that the output is the same
shape as the test predicate. This helper removes that restriction
#+begin_src R :session rsession1 :tangle ~/stats-781/src/depends.R
  #' scheme-like if expression, without restriction of returning same-size table of .test, as ifelse() does
  #'
  #' @param .test predicate to test
  #'
  #' @param true expression to return if .test evals to TRUE
  #'
  #' @param false expression to return if .test evals to TRUE
  #'
  #' @return either true or false
  ifexp <- function(.test, true, false){
    if (.test) {
      return(true)
    } else {
      return(false)
    }
  }
#+end_src
*** Filetype from extension
A helper function to attain the document filetype from the file name.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/depends.R
  #' Get filetype
  #'
  #' @param filepath string filepath of document
  #'
  #' @return filetype (string) - NA if no extension
  get_filetype <- function(filepath){
    filepath %>%
      basename %>%
      str_extract('[a-zA-Z0-9]+\\.[a-zA-Z0-9]+$') %>% #ensure filename.extension form
      str_extract('[a-zA-Z0-9]+$')                  #extract extension
  }
#+end_src
*** Mark the text column of a table
A helper function to determine and mark the text column of a table
#+begin_src R :session rsession1 :tangle ~/stats-781/src/depends.R
  #' Interactively determine and automatically mark the text column of a table
  #'
  #' @param data dataframe with column requiring marking
  #'
  #' @return same dataframe with text column renamed to "text"
  table_textcol <- function(data){
  cols <- colnames(data)
  print("Please enter the number of the column you want selected for text analytics")
  print(cols)
  textcol_index <- get_valid_input(as.character(1:ncol(data))) %>%
    as.integer 
  textcol <- cols[textcol_index]  
  data %>%
      rename(text = !! sym(textcol))
  }
#+end_src
*** Validate User Input
A helper function to get valid user input
#+begin_src R :session rsession1 :tangle ~/stats-781/src/depends.R
  #' helper function to get valid input (recursively)
  #'
  #' @param options vector of options that valid input should be drawn from
  #'
  #' @return readline output that exists in the vector of options
  get_valid_input <- function(options, init=TRUE){
    input <- ifelse(init,
		    readline(),
		    readline(prompt = "Invalid option. Please try again: "))
    ifelse(input %in% options,
	   input,
	   get_valid_input(options, init=FALSE))
  }
#+end_src
* Program Layer: Preparation
Here I lay out the preparation layer in detail. The culmination of all
preparation functions is one wrapper, requesting the possible
preparation features, and outputting a final tibble that is worked on
by the next insight layer.

Multiple documents are input the same as singular, though with an
additional "document" column that can be grouped upon.

The following sections detail the components of text preparation.
** TODO [#C] Importing
A variety of filetypes are able to be imported, with one wrapper
function intelligently determining the appropriate import function
from the file extension. Files with unrecognised extensions are
treated as plaintext. Importantly, as we are working in a tidy
paradigm, everything is imported as a tibble, with plaintext being one
line per row, and tabular data maintaining the original form. Tabular
data requires the specification of which column is the text column for
analytics. All imports have a document ID, which is an identifier column.
*** Import .txt
Plaintext is the most important and simplest to work with of all text
representations; entire operating systems are built around the
concept. 
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' Import text file 
  #'
  #' @param filepath a string indicating the relative or absolute
  #'     filepath of the file to import
  #'
  #' @return tibble of each row corrresponding to a line of the text
  #'     file, with the column named "text"
  import_txt <- function(filepath){
      read_lines(filepath) %>%
	  tibble(text=.)
  }
#+end_src
*** Import .csv
CSV is a plaintext tabular format, with columns typically delimited by
commas, and rows by new lines. A particular point of difference in the
importation of tabular data and regular plaintext is that the text of
interest for the analysis should be (as per tidy principles) in one
column, with the rest being additional information that can be used
for grouping or filtering. Thus, additional user input is required, in
the specification of which column is the text column of interest.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' Import csv file
  #'
  #' @param filepath a string indicating the relative or absolute
  #'     filepath of the file to import
  #'
  #' @return tibble of each row corrresponding to a line of the text
  #'     file, with the column named "text"
  import_csv <- function(filepath){
    read_csv(filepath) %>%
    table_textcol()
  }
#+end_src
*** Import Excel
Unfortunately, much data exists in the Microsoft Excel format, but
this must be catered for. As tabular data, it is treated equivalently
to csv.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' Import excel file
  #'
  #' @param filepath a string indicating the relative or absolute
  #'     filepath of the file to import
  #'
  #' @return tibble of each row corrresponding to a line of the text
  #'     file, with the column named "text"
  import_excel <- function(filepath){
      read_excel(filepath) %>%
	  table_textcol()
  }
#+end_src
*** TODO Import Gutenberg
Project Gutenberg is an online library containing, at the time of
writing, over 57,000 items, primarily plaintext ebooks. This is a
goldmine of text ripe for analysis, and once the basic frontend is
complete, I will dedicate some thought to the in-app importation of
Gutenberg texts
*** Import
The base wrapper function takes in the filename, and other relevent
information, handling the importation process. It also stamps in the
name of the document as a column
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' Base case for file import
  #'
  #' @param filepath string filepath of file for import
  #'
  #' return imported file with document id
  import_base_file <- function(filepath){
    filetype <- get_filetype(filepath)
    filename <- basename(filepath)
    if (filetype == "csv"){
      imported <- import_csv(filepath)
    } else if (filetype == "xlsx" | filetype == "xls") {
      imported <- import_excel(filepath)
    } else {
      imported <- import_txt(filepath)
    }
    imported %>%
      mutate(doc_id = filename)
  }
#+end_src
The base file import is generalised to multiple files with a multiple
import function: this will be our sole import function (until we get
direct Gutenburg import)
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  import_files <- function(){
    filepaths <- tk_choose.files()
    filepaths %>%
      map(import_base_file) %>%
      bind_rows
  }
#+end_src
** Formatting
To work in a tidy paradigm, following the lead of tidytext, we
separate and ID by token. To do this, we take the line ID, the
sentence ID, then the word ID, producing a dataframe that takes the
following form:

| line_id | sentence_id | word_id | word  |
|---------+-------------+---------+-------|
|       1 |           1 |       1 | the   |
|       1 |           1 |       2 | quick |
|       2 |           1 |       3 | brown |

The reason for the ID columns is the preservation of the structure of
the text; If required, the original text can be reconstructed in
entirety, sans minor punctuation differences. The =unnest_tokens=
function from tidytext doesn't play as expected with groups at
present, so much of grouping is (not ideally) taking place internally
in the first =group_modify=. When I have the luxury of time, I will
try to optimise this.

[2019-07-17 Wed]: removed first group modify; unnecessary now that
grouping has been shifted to take place afterwards

#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' formats imported data into an analysis-ready format
  #'
  #' @param data a tibble formatted with a text and (optional) group
  #'     column
  #'
  #' @return a tibble formatted such that columns correspond to
  #'     identifiers of group, line, sentence, word (groups ignored)
  format_data <- function(data){
      data %>%
	  mutate(line_id = row_number()) %>% 
		  unnest_tokens(output = sentence, input = text, token = "sentences", to_lower = FALSE) %>%
		  mutate(sentence_id = row_number()) %>%
	  group_by(sentence_id, add=TRUE) %>%
	  group_modify(~ {
	      .x %>%
		  unnest_tokens(output = word, input = sentence, token = "words", to_lower=FALSE) %>%
		  mutate(word_id = row_number())
	  }) %>%
	  ungroup_by("sentence_id")
  }
#+end_src
** TODO [#B] Filtering
Filtering has to be done with code at present, but the intention is
that once I have a frontend up, it's design will inform an interactive
filter. After some initial analytics have been done in the insight
layer, then preparation can be returned to and the text can be
filtered on based on the analytics.
** Lemmatisation
Lemmatisation is effectively the process of getting words into
dictionary form. It is actually a very complex, stochastic procedure,
as natural languages don't follow consistent and clear rules all the
time. Hence, models have to be used. Despite the burden, it is
generally worthwhile to lemmatise words for analytics, as there are
many cases of words not being considered significant, purely due to
taking so many different forms relative to others. Additionally,
stopwords work better when considering just the lemmatised form,
rather than attempting to exhaustively cover every possible form of a
word. [[https://github.com/trinker/textstem/][textstem]] is an R package allowing for easy lemmatisation, with
it's function =lemmatize_words= transforming a vector of words into
their lemmatised forms (thus being compatible with =mutate= straight
out of the box). Udpipe was another option, but it requires
downloading model files, and performs far more in depth linguistic
determinations such as parts-of-speech tagging, that we don't need at
this point. Worth noting is that, like stopwords, there are different
dictionaries available for the lemmatisation process, but we will use
the default, as testing has shown it to be the simplest to set up and
just as reliable as the rest.
** Stemming
Stemming is far simpler than lemmatisation, being the removal of word
endings. This doesn't require as complex a model, as it is
deterministic. It is not quite as effective, as the base word ending
is not tacked back on at the end, so we are left with word stumps and
morphemes. However, it may sometimes be useful when the lemmatisation
model isn't working effectively, and textstem provides the capability
with =stem_words=
** Stopwords
Stopwords are syntactical features of text that are superfluous and
get in the way of text analytics. Typical examples include articles
and pronouns, like "the", "to", "I", etc. They would clutter the
output of insights such as word frequency. We need a way of generating
a list of stopwords, from both a default source, as well as allowing
the user to add their own stopwords. =get_sw= performs that, detailed
below.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' Gets stopwords from a default list and user-provided list
  #'
  #' @param sw_list a string name of a stopword list, one of "smart",
  #'     "snowball", or "onix"
  #'
  #' @param addl user defined character vector of additional stopwords,
  #'     each element being a stopword
  #'
  #' @return a tibble with one column named "word"
  get_sw <- function(lexicon = "snowball", addl = NA){
    addl_char <- as.character(addl)
    get_stopwords(source = lexicon) %>%
      select(word) %>%
      bind_rows(., tibble(word = addl_char)) %>%
      na.omit() %>%
      as_vector %>%
      tolower() %>%
      as.character()
  }
#+end_src
The status of the stopwords are then added to the data with =determine_stopwords=
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' determine stopword status
  #'
  #' @param .data vector of words
  #'
  #' @param ... arguments of get_sw
  #'
  #' @return a dataframe equivalent to the input dataframe, with an additional stopword column
  determine_stopwords <- function(.data, ...){
    sw_list <- get_sw(...)
    .data %in% sw_list
  }
#+end_src
** Object Preparation
The =preparation= wrapper takes all combinations of stopwords and
lemmatisation options and intelligently connects them for the "insight
column", which the insight is performed upon. For the purpose of
standard interoperability with, e.g., ggpage, we name this column
"text"

The gnarly =ifexp= taking up the heart of the function encodes the
logic involving the interaction of stopwords and lemmatisation:

|                 | Stopwords True                                                                                    | Stopwords False                      |
| Lemmatise True  | Lemmatise, determine stopwords on lemmatisation, perform insight on lemmas sans stopwords         | Lemmatise, perform insight on lemmas |
| Lemmatise False | Determine stopwords on original words (no lemmatisation), perform insight on words sans stopwords | Perform insight on original words    |

#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  text_prep <- function(.data, lemmatize=TRUE, stopwords=TRUE, sw_lexicon="snowball", addl_stopwords=NA){
    formatted <- .data %>%
      format_data()

    text <- ifexp(lemmatize,
			 ifexp(stopwords,
			       mutate(formatted, lemma = tolower(lemmatize_words(word)),
				      stopword = determine_stopwords(lemma, sw_lexicon, addl_stopwords),
				      text = if_else(stopword,
							    as.character(NA),
							    lemma)),
			       mutate(formatted, lemma = tolower(lemmatize_words(word)),
				      text = lemma)),
			 ifexp(stopwords,
			       mutate(formatted, stopword = determine_stopwords(word, sw_lexicon, addl_stopwords),
				      text = if_else(stopword,
							    as.character(NA),
							    word)),
			       mutate(formatted, text = word)))
    return(text)
  }
#+end_src
** Sectioning
Plaintext, as might exist as a Gutenberg Download, differs from more
complex representations in many ways, including a lack of sectioning -
Chapters require a specific search in order to jump to them. Here, I
compose a closure that searches and sections text based on a Regular
Expression intended to capture a particular section. Several functions
are created from that. In time, advanced users could be given the
option to compose their own regular expressions for sectioning.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
  #' creates a search closure to section text
  #'
  #' @param search a string regexp for the term to seperate on, e.g. "Chapter"
  #'
  #' @return closure over search expression 
  get_search <- function(search){
    #' section based on search result 
    #' 
    #' @param .data vector to section
    #'
    #' @return vector of same length as .data with section numbers
    function(.data){
      .data %>%
	str_detect(search) %>%
	accumulate(sum, na.rm=TRUE)
      }
  }

  get_chapters <- get_search("^[\\s]*[Cc][Hh][Aa]?[Pp][Tt]([Ee][Rr])?")
  get_parts <- get_search("^[\\s]*[Pp]([Aa][Rr])?[Tt]")
  get_sections <- get_search("^[\\s]*([Ss][Ss])|([Ss][Ee][Cc][Tt][Ii][Oo][Nn])")
#+end_src

How to implement sectioning in a way that fits in a shiny UI is still
very much TBC. Presumably, after object preparation, the option to
section would appear, followed by a group selection option. I will
implement these only after implementing the shiny app.
** TODO [#C] Grouping 
Grouping is a killer feature of our app. The intention is to run a
=group_by= dplyr command in the wrapper over user-specified groups,
and all further insights and visualisations are performed groupwise.
This allows for immediate and clear comparisons.

Like filtering, after some initial analytics have been done in the
insight layer, then preparation can be returned to and the text can be
groupedon based on the analytics.

Also needed, but surprisingly missing from dplyr, is an "ungroup_by"
function, that allows specifice groups to be removed. Currently
standard evaluation only, will switch to NSE when time allows

TODO:
- [ ] Make =ungroup_by= NSE
#+begin_src R :session rsession1 :tangle ~/stats-781/src/prep-for-insight.R
#' helper function to ungroup for dplyr. functions equivalently to
#' group_by() but with standard (string) evaluation
ungroup_by <- function(x,...){
    group_by_at(x, group_vars(x)[!group_vars(x) %in% ...])
}
#+end_src
* Program Layer: Insight
Insight is the meat of this package. After some initial resistance, I
have decided to jump all-in with tidyverse-style transformations,
especially for the non-destructive editing, as an immutable functional
programming paradigm suits such functions. Insight may be divided into
word insight, and higher-level (aggregated) insights. The higher level
insights include sentence and document level insights, such as
sentence sentiment, tf-idf, etc. Importantly for the document level
insights is that our program doesn't necessarily have to work purely
on documents - any identifying column could potentially stand in.

At present, all insight functions haven't yet been tested with the new
output of the Preparation layer. I want to make the following changes
to all of them for this to be effective:

TODO:
- [X] Have all insight functions work on vector input and output, so
  as to work with =mutate=
- [ ] Ensure correctness of output under grouping
** Word Insight
*** Word Frequency
Frequencies of words are useful in getting an understanding of what
terms are common in a text. This is one insight in particular that
requires stopwords to have been previously removed, otherwise the top
words will always be syntactical glue, such as articles
#+begin_src R :session rsession1 :tangle ~/stats-781/src/word-insight.R
  #' Determine word frequency
  #'
  #' @param .data character vector of words
  #'
  #' @return numeric vector of word frequencies
  word_freq <- function(.data){
    ## .data %>%
    ##   map_dbl(function(x){sum(x == .data, na.rm = TRUE)})
    .data %>%
    enframe %>%
    add_count(value) %>%
    mutate(n = if_else(is.na(value),
		       as.integer(NA),
		       n))  %>%
    pull(n)
  }
#+end_src
*** [#C] Bigrams [0/1]
Bigrams are two words that occur in sequence. For example, in the
phrase, "The quick brown dog.", the following bigrams exist: "The
quick", "quick brown", "brown dog". This can be generalised to any
number of sequential words as /n-grams/. They are useful in text
analytics to determine word sequences, as well as common adverb-verb
and adjective-noun pairs. This exists partly between word and
aggregate insight, but by measure is closer to the word-level.

When we attain the bigrams, we can use the word frequency function
defined previously to attain a bigram frequency.

TODO:
- [ ] generalise to n-grams (make closure, have bigrams as special
  case)

I determine bigrams by matching the vector of words with itself, sans
the first element in the second list.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/word-insight.R
  #' Determine bigrams
  #'
  #' @param .data character vector of words
  #'
  #' @return character vector of bigrams
  get_bigram <- function(.data){
    1:length(.data) %>%
      map_chr(index_bigram, .data, .data[-1])
	  ## mutate(bigram = paste(word, lead(word)))%>%
	  ## add_count(bigram) %>%
	  ## rename(bigram_freq = n)
  }
#+end_src

However, it is more complex than that; we need a way to deal with NA
values. This beautiful recursive function (designed by myself) does
just that:

#+begin_src R :session rsession1 :tangle ~/stats-781/src/word-insight.R
  #' get bigram at index i of list1 & 2
  #'
  #' @param i numeric index to attain bigram at
  #'
  #' @param list1 list or vector for first bigram token
  #'
  #' @param list2 list or vector for second bigram token
  #'
  #' @return bigram of list1 and list2 at index i, skipping NA's
  index_bigram <- function(i, list1, list2){
    ifelse(length(list2) < i | is.na(list1[i]),
	    as.character(NA),
    ifelse(!(is.na(list1[i]) | is.na(list2[i])),
	   paste(list1[i], list2[i]),
	   index_bigram(i,list1, list2[-1])))
  }
#+end_src

It works as follows; if our list appears as (1 2 X 4 5 X 7 X), we
expect the following bigrams: ((1 2) (2 4) X (4 5) (5 7) X X X), due
to bigrams taking the lead of list1 as list2, as per the following
table:

| list1 | list2 | bigram |
|-------+-------+--------|
|       |     1 |        |
|     1 |     2 | 1 2    |
|     2 |     X | 2 4    |
|     X |     4 | X      |
|     4 |     5 | 4 5    |
|     5 |     X | 5 7    |
|     X |     7 | X      |
|     7 |     X | X      |
|     X |       | X      |

#+begin_src R :session rsession1 :tangle no
x <- c(1, 2, NA, 4, 5, NA, 7, NA)
get_bigram(x)
#+end_src

#+RESULTS:
| 1 2 |
| 2 4 |
| nil |
| 4 5 |
| 5 7 |
| nil |
| nil |
| nil |

*** Key Words (TextRank)
Key words are another killer feature of this app. The algorithm is
explained previously. The =textrank= package is used to perform
textrank. Of note is that all words other than stopwords (indicated by
NA) are relevent, but the standard algorithm works on data that has
had POS tagging, typically assessing only nouns and adjectives. We
don't do that here as the processing burden for POS tagging is
enormous, though it may be implemented in the future.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/word-insight.R
  #' Determine textrank score for vector of words
  #'
  #' @param .data character vector of words
  #'
  #' @return vector of scores for each word
  keywords_tr <- function(.data){
    relevent <- !is.na(.data)
    tr <- textrank_keywords(.data, relevent, p=+Inf)
    score <- tr$pagerank$vector %>% enframe
    data <- .data %>% enframe("number", "name")
    full_join(data, score, by="name") %>%
      pull(value)
  }
#+end_src
*** Word Sentiment [1/1]
Sentiment has been discussed earlier. Effectively, for any text
analytics it is essential. There are numerous sentiment dictionaries,
but we will use AFINN for the nice numeric properties it has, allowing
for statistics on them. Categorical dictionaries will be implemented
later.

TODO:
- [X] Include option for additional dictionaries
#+begin_src R :session rsession1 :tangle ~/stats-781/src/word-insight.R
  #' Determine AFINN sentiment of words
  #'
  #' @param .data vector of words
  #'
  #' @param lexicon sentiment lexicon to use, based on the corpus
  #'   provided by tidytext
  #' 
  #' @return vector with sentiment score of each word in the vector
  word_sentiment <- function(.data, lexicon="AFINN"){
    data <- enframe(.data, "number", "word")
      sentiments %>%
	filter(lexicon == !! lexicon) %>%
	select(word, score) %>%
	right_join(data, by="word") %>%
	pull(score)
  }
#+end_src
*** TODO [#B] Word Correlation
This is the word-level insight that will be the most difficult to
perform, due to my requirements that the dataframe remains tidy and
lossless. The only way I can conceive of doing this is by adding
columns for each distinct word, giving correlations there. The best
form of visualisation would be individual words with their scores, a
correlation matrix for some words, or a table and search like the one
Cassidy created.
** TODO [#A] Aggregate Insight
This should work effectively the same as the word-level insight,
however the wrapper may have to be different. This is TBC. I think an
"aggregate on ..." user option would be useful
*** Word Count
Word count on some aggregate group is following the pattern I have
been noticing where the simpler a function is, the more analytical
power it seems to give. This may be generalised in the future to give
a nested aggregate count (e.g. sentences/paragraph, lines/document
etc.)

Note in the following function the near canonical example of
split-apply-combine, or MapReduce style. This allows for performance
gains in parallel processing, ideal for the large datasets we
typically work with in text analytics.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/sentence-insight.R
  #' Determine the number of words at each aggregate level
  #'
  #' @param .data character vector of words
  #'
  #' @param aggregate_on vector to split .data on for insight
  #'
  #' @return vector of number of words for each aggregate level, same
  #'   length as .data
  word_count <- function(.data, aggregate_on){
    split(.data, aggregate_on) %>%
      map(function(x){rep(length(x), length(x))}) %>%
      combine()
  }
#+end_src
*** Key Sentence (LexRank)
Often keywords aren't very explanatory on their own; patterns only
really develop in aggregate. We use lexrank as the algorithm for
key-sentences, as textrank takes too long, though lexrank seems to
take just as long at high /n/ - it may be worth exploring the option
of textrank again.

Testing shows a performance of around 3-4 mins for ~30,000 words of
text aggregated of ~3000 sentences. Not bad for something graph based,
but a warning will be required at the user end.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/sentence-insight.R
  #' get score for key sentences as per Lexrank
  #'
  #' @param .data character vector of words
  #'
  #' @param aggregate_on vector to aggregate .data over; ideally, sentence_id
  key_sentences <- function(.data, aggregate_on){
    ## prepare .data for lexrank
    base <-  tibble(word = !! .data, aggregate = aggregate_on)
    aggregated <- base %>%
      group_by(aggregate) %>%
      na.omit() %>%
      summarise(sentence = paste(word, collapse = " ")) %>%
      mutate(sentence = paste0(sentence, "."))
    ## lexrank
    lr <- aggregated %>%
      pull(sentence) %>%
      lexRank(., n=length(.),removePunc = FALSE, returnTies = FALSE,
	      removeNum = FALSE, toLower = FALSE, stemWords = FALSE,
	      rmStopWords = FALSE, Verbose = TRUE)
    ## match lexrank output to .data
    lr %>%
      distinct(sentence, .keep_all = TRUE) %>% 
      full_join(aggregated, by="sentence") %>%
      full_join(base, by="aggregate") %>%
      arrange(aggregate) %>%
      pull(value)
  }

#+end_src
*** Aggregate Sentiment
Like the added context that key sentences bring over key words, a
similar situation is true of sentiment. I'll make it so that it can
deliver any statistic of a sentence; mean, median, variance etc.
Importantly, it will only work with numeric sentiment lexicons, in our
case, AFINN.
#+begin_src R :session rsession1 :tangle ~/stats-781/src/sentence-insight.R
  #' Get statistics for sentiment over some group, such as sentence.
  #'
  #' @param .data character vector of words
  #'
  #' @param aggregate_on vector to aggregate .data over; ideally,
  #'   sentence_id, but could be chapter, document, etc.
  #'
  #' @param statistic function that accepts na.rm argument; e.g. mean,
  #'   median, sd.
  aggregate_sentiment <- function(.data, aggregate_on, statistic){
    enframe(.data, "nil1", "word") %>%
      bind_cols(enframe(aggregate_on, "nil2", "aggregate")) %>%
      select(word, aggregate) %>%
      mutate(sentiment = word_sentiment(word)) %>%
      group_by(aggregate) %>%
      mutate(aggregate_sentiment =
	       (function(.x){
		 rep(statistic(.x, na.rm = TRUE), length(.x))
	       })(sentiment)) %>%
      pull(aggregate_sentiment)
  }
#+end_src
*** TODO [#A] Term Frequency - Inverse Document Frequency (tf-idf)
*** TODO [#A] Topic Modelling
** TODO [#B] Wrapper
The insights of choice can all be combined into a wrapper function,
taking the forms and arguments of the insights and applying those
chosen.

TODO:
- [ ] Take multiple insights
#+begin_src R :session rsession1 :tangle ~/stats-781/src/word-insight.R
#' perform group-aware operation on the standard dataframe
#'
#' @param std_tib the standard dataframe given as the output of the format_data function
#'
#' @param operation insight function to be performed on the dataframe
#'
#' @return grouped output from the operation
get_insight <- function(std_tib, operation){
    std_tib %>%
        group_modify(~ {
            .x %>%
                operation
        })
}
#+end_src
* Program Layer: Visualisation
I have grouped visualisations by their output intention, rather than
their implementation, as an ends-based focus, with the means being
mere details. The following are the most useful visualisations. A
present issue with visualisation is how grouping is performed; If I
want to have a set of charts separated by group, performing by group
creates as many separate charts as there are groups, as separate
graphics. I want to make use of =facet_wrap= from ggplot, which
requires some maneuvering with a wrapper function
** Distribution [1/4]
*** CLOSED Density
    CLOSED: [2019-07-13 Sat 23:20]
#+begin_src R :session rsession1 :tangle ~/stats-781/src/vis-insight.R
#' output a histogram of the distribution of some function of words
#'
#' @param std_tib the standard dataframe, modified so the last column
#'     is the output of some insight function (eg. output from
#'     word_freq)
#'
#' @param insight_col string name of the column insight was
#'     performed on
word_dist <- function(std_tib, insight_col){
std_tib %>%
    ggplot(aes(x = !! sym(insight_col))) +
    geom_density()
}
#+end_src
*** TODO Histogram
*** TODO Boxplot
*** TODO Ungrouped Boxplot
** TODO Rank
** TODO Score
*** TODO [#B] Barplot [0/2]
There are issues with the barplot, as documented by:
#+begin_quote
[2019-07-01 Mon] After implementing grouping, the issue with arranging
bars in a barplot by rank within each group is that ggplot arranges
bars through the ordering of factor levels. The problem is that each
instance of a word in every group shares the same level ordering, so
while a word may rank highly overall, but less than others in a
particular group, it will retain the high ordering overall in the
facet for that group, leading to potential confusion
#+end_quote

[2019-07-15 Mon]: https://juliasilge.com/blog/reorder-within/ may be a solution

Which I do want to fix, though it isn't necessarily /incorrect/.
Additionally, this function takes too many arguments.

TODO:
- [ ] Find way to better order score
- [ ] Find way to lower number of arguments
#+begin_src R :session rsession1 :tangle ~/stats-781/src/vis-insight.R
#' output a bar graph of the top words from some insight function
#'
#' @param std_tib the standard dataframe, modified so the last column
#'     is the output of some insight function (eg. output from
#'     word_freq)
#'
#' @param insight_name string name of the column insight
#'     was performed on
#' 
#' @param insight_col string name of the column insight was
#'     outputted to
#'
#' @param n number of bars to display
#'
#' @param desc bool: show bars in descending order
#'
word_bar <- function(std_tib, insight_name, insight_col,
                     n = 15, desc = TRUE){
    dist <- std_tib %>%
        distinct(word, .keep_all=TRUE)
    if (desc) {
        arr <-  arrange(dist, desc(!! sym(insight_col)))
    }else{
        arr <- arrange(dist, !! sym(insight_col))
    }
    arr %>%
        group_modify(~{.x %>% head(n)}) %>%
        ungroup() %>%
        mutate(!! sym(insight_name) := fct_reorder(!! sym(insight_name),
                                                   !! sym(insight_col),
                                                   .desc = desc)) %>%
        ggplot(aes(x = !! sym(insight_name))) +
        geom_col(aes(y = !! sym(insight_col)))
}
#+end_src
** TODO Structure
*** TODO [#A] ggpage
 ggpage allows us to show off the importance of our non-destructive
 editing - the original document can be displayed, with the insights
 highlighted. There was more discussion on ggpage under an earlier section.
** TODO [#C] Relation
*** Correlation Matrix
** TODO [#B] Wrapper
This is an attempt to create a group-aware visualisation,
automatically facetting by group. I feel like it is not ideal, though
haven't had any major bugs with it yet
#+begin_src R :session rsession1 :tangle ~/stats-781/src/vis-insight.R
#' create a group-aware visualisation
#'
#' @param std_tib the standard dataframe, modified so the last column
#'     is the output of some insight function (eg. output from
#'     word_freq)
#'
#' @param vis visualisation function
#'
#' @param ... visualisation function arguments
get_vis <- function(std_tib, operation, ...){
    if (is_grouped_df(std_tib)){
        grouping <- group_vars(std_tib)
        std_tib %>%
            operation(...) + facet_wrap(syms(grouping), scales="free_x", labeller = "label_both") #
    } else {
        std_tib %>%
            operation(...)
    }
}
#+end_src
* Testing / Demonstration
** Source
 #+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
   source("../src/depends.R")
   source("../src/prep-for-insight.R")
   source("../src/word-insight.R")
   source("../src/sentence-insight.R")
   source("../src/vis-insight.R")
 #+end_src
** Ungrouped
*** Preparation
#+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
  imported <- import_files()
  lemmatize <- TRUE
  stopwords <- TRUE
  sw_lexicon <- "snowball"
  addl_stopwords <- NA
  data <- text_prep(imported, lemmatize, stopwords, sw_lexicon, addl_stopwords)
#+end_src
*** Insights
#+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
  insighted <- data %>%
    mutate(
    word_freq = word_freq(text),
    bigram = get_bigram(text),
    bigram_freq = word_freq(bigram),
    word_sentiment = word_sentiment(text),
    word_count_sentence = word_count(text, sentence_id),
    mean_aggregate_sentiment_sentence = aggregate_sentiment(text, sentence_id, mean),
    sd_aggregate_sentiment_sentence = aggregate_sentiment(text, sentence_id, sd)
    )
#+end_src
*** Visualisation
#+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
  # … with 29,944 more rows, and 13 more variables: line_id <int>, word1 <chr>,
  #   word_id <int>, lemma <chr>, stopword <lgl>, text <chr>, word_freq <int>,
  #   bigram <chr>, bigram_freq <int>, word_sentiment <int>,
  #   word_count_sentence <int>, mean_aggregate_sentiment_sentence <dbl>,
  #   sd_aggregate_sentiment_sentence <dbl>

  insighted %>%
    pull(word) %>%
    ggpage_build() %>%
    bind_cols(insighted) %>%
    ggpage_plot(aes(colour=mean_aggregate_sentiment_sentence)) +
    scale_color_gradient2()

  insighted %>%
    pull(word) %>%
    ggpage_build() %>%
    bind_cols(insighted) %>%
    ggpage_plot(aes(colour=word_count_sentence))



#+end_src
** Grouped
*** Preparation
We import a file downloaded from Project Gutenberg, and run through
some basic preparation, with additional stopwords to be removed
#+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
  imported <- import_files()
  lemmatize <- TRUE
  stopwords <- TRUE
  sw_lexicon <- "snowball"
  addl_stopwords <- NA
  prepped <- text_prep(imported, lemmatize, stopwords, sw_lexicon, addl_stopwords)
  sectioned <- prepped %>% mutate(chapter = get_chapters(text))
  data <- sectioned ## %>%
    ## group_by(doc_id,chapter)

  ## .data <- data$text
  ## aggregate_by <- data$chapter
#+end_src
*** Insights
#+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
wf <- std_tib %>%
    get_insight(word_freq)

bf <- std_tib %>%
    get_insight(bigram_freq)

kw <- std_tib %>%
    get_insight(keywords_tr)

ws <- std_tib %>%
    get_insight(word_sentiment_AFINN)
#+end_src
*** Visualisation
#+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
wf %>%
    get_vis(word_dist, "word_freq")

wf %>%
    get_vis(word_bar, "word", "word_freq")

kw %>%
    get_vis(word_bar, "word", "rank", desc=FALSE)

ws %>%
    get_vis(word_dist, "score")
#+end_src

** ggpage
ggpage is a very interesting piece of visualisation, tested here. Once I build up the correct preparation format, I will perform more intensive testing here
#+begin_src R :session rsession1 :tangle ~/stats-781/test/tests.R
filename <- "../data/raw/11-0.txt"

imported <- import_txt(filename) 

imported %>%
    ggpage_build() %>%
    filter(page == 1) %>%
    ggpage_plot()

imported %>%
    ggpage_build() %>%
    filter(page == 1) %>%
    get_insight(word_freq) %>%
    ggpage_plot(aes(fill=word_freq))

stopwords <- get_sw()

imported <- import_txt(filename) %>%
    format_data() %>%
    remove_stopwords(stopwords) %>%
    reconstruct()

imported %>%
    ggpage_build() %>%
    get_insight(word_freq) %>%
    ggpage_plot(aes(fill=word_freq))

imported %>%
    ggpage_build() %>%
    get_insight(keywords_tr) %>%
    ggpage_plot(aes(fill=rank))

imported %>%
    ggpage_build() %>%
    get_insight(word_sentiment_AFINN) %>%
    ggpage_plot(aes(fill=score)) +
    scale_fill_gradient2(low = "red", high = "blue", mid = "grey", midpoint = 0)
#+end_src
* Shiny Frontend
