---
title: Free-Response Analytics
author: Jason Cairns
---
# Initialisation

```{r}
library(tidyverse)
library(tidytext)
library(lexRankr)
library(textrank)
library(widyr)
library(topicmodels)
library(tictoc)
```

```{r}
setwd("../data/")
source("data_import.R")
```

```{r}
tokenised_cancer_soc <-
  cancer_soc %>%
  select(sq6a) %>%
  mutate(id = row_number()) %>%
  unnest_tokens(word, sq6a) %>%
  na.omit
```

```{r}
no_stop_cancer_soc <-
  tokenised_cancer_soc %>%
  anti_join(get_stopwords())
```

# Within-text analytics

## Common Terms
```{r}
no_stop_cancer_soc %>%
  count(word, sort = TRUE)
```

## Sentiment Analysis

some quick sentiment analysis using NRC. Let's see what NRC has available:

```{r}
get_sentiments("nrc") %>%
  distinct(sentiment)
```

```{r}
nrc_trust <-
  get_sentiments("nrc") %>%
  filter(sentiment == "trust")

no_stop_cancer_soc %>%
  semi_join(nrc_trust) %>%
  count(word, sort = TRUE)
```

## Summarisation

There are too many responses, each being too short, for a summarisation for each response to be useful.

## Keywords

Same as above

## Term Correlations

### Bigrams

```{r}
cancer_soc_bigrams <-
  cancer_soc %>%
  select(sq6a) %>%
  mutate(id = row_number()) %>%
  unnest_tokens(bigram, sq6a, token = "ngrams", n = 2) %>%
  na.omit()

bigrams_separated <- cancer_soc_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# new bigram counts:
bigram_counts <- bigrams_united %>% 
  count(bigram, sort = TRUE)

bigram_counts
```

### Pairwise

```{r}
word_cors <- tokenised_cancer_soc %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, id, sort = TRUE)

word_cors
```

This is probably far more useful than bigrams.

# Between-text analytics

## tf-idf 

This is a measure of the important words within each response, ignoring those
that are common throughout - may be not so useful in this analysis? too many observations, but may be useful under small n grouping variables.

```{r}
cancer_soc_words <-
  cancer_soc %>%
  select(sq6a) %>%
  mutate(id = row_number()) %>%
  unnest_tokens(word, sq6a) %>%
  count(id, word, sort = TRUE)

total_words <-
  cancer_soc_words %>%
  group_by(id) %>%
  summarize(total = sum(n))

tf_idf_cancer_soc <-
  left_join(cancer_soc_words, total_words) %>%
  bind_tf_idf(word, id, n)

tf_idf_cancer_soc %>%
  arrange(desc(tf_idf))
```

## Summarisation

We have a summarisation of the overall common sentences between all responses

```{r}
lexRank(cancer_soc$sq6a)
```

## Keywords 

Found to be surprisingly fast with textrank

```{r}
keyw <-
  textrank_keywords(no_stop_cancer_soc$word)

head(keyw$keywords, 10)
```

## Topic Modelling

```{r}
cancer_soc_word_count <-
  no_stop_cancer_soc %>%
  count(id, word, sort = TRUE)
  

response_dtm <- cancer_soc_word_count %>%
  cast_dtm(id, word, n)
```

```{r}
tic()
response_lda <- LDA(response_dtm, k = 4)
toc()
response_lda
```
LDA extremely fast here - less than a second

```{r}
response_lda_td <- tidy(response_lda)
response_lda_td
```

```{r}
top_terms <- response_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

```{r}
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ topic, scales = "free") +
  theme(axis.text.x = element_text(size = 15, angle = 90, hjust = 1))
```


