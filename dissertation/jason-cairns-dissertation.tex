\documentclass[11pt, a4paper, oneside]{report}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{minted}
\usepackage{booktabs}
\usepackage[UKenglish]{babel}
\usepackage[en-GB,showdow]{datetime2}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{imakeidx} \makeindex
\usepackage[
sorting=none,
hyperref=true,
backend=biber,
style=numeric,
backref=true
]{biblatex}
\addbibresource{references.bib}
\usepackage{todonotes}
\usepackage{pdfpages}
\usepackage{glossaries} \makeglossaries
\usepackage{graphicx}
\usepackage{framed}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{parskip}



\newglossaryentry{term}{ name={term}, description={``a
    word or expression that has a precise meaning in some uses or is
    peculiar to a science, art, profession, or
    subject'\autocite{dictionary:_term_defin_term} --- here text
    analysts have capitalised on the generalisation of ``term'to
    include subcomponents or aggregations of words} }




\begin{document}

% \frontmatter
\begin{titlepage}
  \centering
  \vspace*{2.5cm}
  {\Huge Text Analytics}\\
  \vspace{1.5cm}
  {\Large Jason Peter Cairns}\\
  \vspace{1.5cm}
  Supervised by Chris Wild\\
  \vspace{1.5cm}
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/logo.jpg}
  \end{figure}
  \vspace{1cm}
  Bachelor of Science (Honours)\\
  Department of Statistics\\
  The University of Auckland\\
  New Zealand
\end{titlepage}

\listoftodos

% \begin{abstract}

% \end{abstract}

\chapter*{Acknowledgements}
\label{cha:acknowledgements}

\tableofcontents
\addcontentsline{toc}{chapter}{Listings}
\renewcommand\listoflistingscaption{List of source codes}
\listoflistings
\addcontentsline{toc}{chapter}{Tables}
\listoftables
\addcontentsline{toc}{chapter}{Figures}
\listoffigures

% \mainmatter
\chapter{Introduction}
\label{cha:introduction}

\section{Intention}
\label{sec:intention}

Text Analytics serves to glean insight from a body of text. Within the
broad category of text analytics, we seek to answer questions about
what the text is communicating, what is felt about it, and how this
information is structured. In this dissertation, we demonstrate the
creation of a user-friendly program to perform text analytics
functions using modern R with the Shiny web application framework. In
a literate style, we illustrate top-down the structure of such a
program, as well as the data structures and computational processes
that have established their value for such a program.\todo{should this
  be an abstract?}
  
\section{Background: Text Analytics (incl. examples)}
\label{sec:backgr-text-analyt}
\subsection{common functions: sentiment, summarisation, scoring}
Text Analytics is comprised of a variety of processes and techniques
to extract information from text. The text almost always requires some
initial processing. Some of the following functions have proven
utility, and are expanded upon in \autoref{cha:text-analyt-backgr};

\begin{itemize}
\item Sentiment: In order to answer what emotions are conveyed in a
  text, sentiment analysis is commonly performed. The technique yields
  some measure of what is represented in an emotional sense by the
  text, with a range different methods and their associated outputs
  allowing for different forms of the analysis. Sentiment analysis
  won't pick up the subtle nuances that a human reader would, but
  generally gives reasonable output over the extent of a text.
\item Associated Words: The meaning of a text is dependent on the
  structure between and within words. Looking at how words are
  associated, through correlation, common sequences, visualisation of
  sections, etc.,\ allow for a clear high-level assessment of the
  associations between words. The higher level not only saves
  individual efforts, but will demonstrate any emergent properties
  inherent to a text, in a way that a direct reading won't necessarily
  reveal.
\item Summarisation: Automation of an executive summary, or a list of
  key words, typically falls under the purview of summarisation. The
  primary aim is to rank and select the most ``representative'' words
  or sentences from a text. A few major techniques dominate, being
  somewhat complex in nature. The results are generally surprisingly
  well representative of a text.
\item Feature Counts: The simplest quantitative measure is very often
  the most informative; from simple word counts, to selective counts
  of sentences within groups, counting features can reveal how much
  written weighting is given to various elements, aiding insight into
  both structure and sentiment simultaneously.
\end{itemize}

\subsection{Existing Systems}
There are several existing systems in the field of Text Analytics. The
field was initially nurtured as a sub-field of Computer Science, being
computationally-dependent in nature. More recently, there has been
increasing statistical interest. The existing systems reflect this;
most older text analytics programs were Artificial Intelligence
focussed, being experimental in nature, typically composed in lisp.
More recently, major statistical programs hae been incorporating text
analytic features, with a few smaller text analytics specific programs
appearing. SAS, SPSS, and R are all examples of major statistical
processing systems, with recent additions of text analytics
capabilities. An overview of R packages aiding in text analytics will
be given in \autoref{sec:liter-revi-exist}.

\section{Background: inZight}
\label{sec:background:-inzight}
\subsection{What iNZight is - capabilities, popularity, etc.}
\subsection{how our program fits in - shiny, inzight lite etc.}
Our program will form part of the suite of modules extending iNZight.
It provides a simple GUI interface to rapidly perform common text
analyses. The primary audience are those learning the fundamentals of
text analysis and statistics, which could include students of the
traditional text analytics fields of Statistics and Computer Science,
but can and should include students of Linguistics, Communications, Law,
History, and any other text-based field. Beyond the educational
aspects of the program, it is fully functional for actual use for
general text analysis.
\section{Literature Review (existing packages in R)}
\label{sec:liter-revi-exist}

\subsection{Copy over from notes, flesh out a bit}

\subsection{Praise tidytext book, complain about the package}

\section{Scope of work}
\label{sec:scope-work}
The total scope possible for text analytics is enormous; our time in
creating this program is not, thus it is essential that we limit the
scope. There are two primary areas with which we created the
limitations: Text type, and analysis type.\\

By limiting the forms of text we work with, we can spend less effort on consideration of every single possible import and transformation case, and more time on the actual design of analysis. The simplest means with which to create the limitation exists in allowing only import of particular text files --- in this case, we allow for flat \texttt{.txt} files, as well as tabular \texttt{.csv} and \texttt{.xlsx} files. What we do not provide (though by design leaving open to the future possibility of including) is access in-program to common text sources through their API, such as Twitter or Project Gutenberg.\\

Through focussing on dictionary-based, rather than model-based
analyses, we have avoided much of the associated complexities. An example of this is given in that it is common to categorise words based on their grammatical category, then use models that take this into account. By avoiding that (again, keeping the design flexible enough to allow for this in the future), we have been able to get far more functionality implemented in a shorter amount of time, with the analyses still performing soundly. Additionally, we keep the focus on the general audience, as it is typically more advanced, linguistically-trained users who would make intelligent use of such analyses.

\chapter{Text Analytics Prolusion}
\label{cha:text-analyt-backgr}

\section{overview}
\label{sec:overview}
Most importantly, words must be extracted, serving as the basic unit
of analysis, from which more complex items may be derived.
\subsection{Explain broadness of term}
\subsection{compile glossary from terms here}
\subsection{Areas of text analytics in a data science framework}
\subsection{what we have done}
\subsection{what we haven't done}

\section{terms}
\label{sec:terms}
\gls{term}
\subsection{terms and their centrality}
\subsection{generalisation: n-grams, sentences etc.}

\section{Historical Background}
\label{sec:hist-backgr}

\subsection{computer science vs statistics - reflection in data science}

\section{Processing}
\label{sec:processing}

\subsection{why process}
\subsection{stopwords, lemmatisation etc.}
\subsection{modelling vs db joins - more info in notes}

\section{scores \& statistics}
\label{sec:statistics}

\subsection{why compute scores \& statistics}
\subsection{scoring - tf-idf, word count}
\subsection{Suggestions for further research - more on the statistics of words}
\subsection{recount the book of John text analysis}

\section{Sentiment}
\label{sec:sentiment}

\subsection{why sentiment}
\subsection{Process of sentiment}
\subsection{sentiment modelling vs db joins}
\subsection{our implementation and why}
\subsection{reviews}
\subsection{issues}

\section{Summarisation}
\label{sec:summarisation}

\subsection{why compute summarisations}
\subsection{lexrank, textrank - include notes on lexrank}
\subsection{other methods}
\subsection{reddit bot example}

\section{what we didn't do (yet)}
\label{sec:what-we-didnt}

\subsection{topic modelling}
\subsection{Term correlation}
\subsection{modelling based on linguistic features}

\section{Visualisation}
\label{sec:visualisation-1}

\subsection{talk about score vs structure}
\subsection{complain about tag clouds}
\subsection{talk about ggpage}
\subsection{discuss our experimentations with some alternative visualisations}
\chapter{Program Structure \& Development}
\label{cha:program-structure}

\subsection{why R}
\subsection{Why Shiny}
\subsection{why tidyverse}
\subsection{Git}
\subsection{possible future: datatables, futures, etc.}
\subsection{why functional}
\subsection{Why lossless data}

\section{Program Architecture}
\label{sec:program-architecture-1}

\subsection{Why structure it like it has been}
\subsection{make graph of architecture}
\subsection{Describe package and package creation}
\subsection{following three sections copy and paste from the notes - buffing up as necessary}
\subsection{include screenshots}

\section{Preparation}
\label{sec:import}
The first step in all text analysis is to import the text data and
capture it into a data structure that reasonable analysis can be
performed on.

\subsection{Importing}
\label{sec:importing}

Text must first be brought in from an outside source to be useful for
the program. The import functions are such that all text from
different files exist in dataframes of equivalent structure. The
primary differences are that each row of an imported \texttt{.txt}
file corresponds to a single line, whereas each row of an imported
tabular file corresponds to the row of the tabular file. Importantly
for tabular files, the column of the text intended for analysis must
be given the header of ``text'' prior to import.

\subsubsection{Import .txt}
\label{sec:org0ff5f7d}
The following is the simple function used in the import of \texttt{.txt} files:
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos]{r}
#' Import text file 
#'
#' @param filepath a string indicating the relative or absolute
#'     filepath of the file to import
#' 
#' @return a [tibble][tibble::tibble-package] of each row
#'   corrresponding to a line of the text file, with the column named
#'   "text"
import_txt <- function(filepath){
  readr::read_lines(filepath) %>%
    tibble::tibble(text=.)
}
\end{minted}
\subsubsection{Import .csv}
\label{sec:orgb2d558e}
CSV is a plaintext tabular format, with columns typically delimited by
commas, and rows by new lines. A particular point of difference in the
importation of tabular data and regular plaintext is that the text of
interest for the analysis should be (as per tidy principles) in one
column, with the rest being additional information that can be used
for grouping or filtering. Thus, additional user input is required, in
the specification of which column is the text column of interest. The
following function is effectively just a wrapper around
\mintinline{r}{readr::read_csv()}
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos]{r}
#' Import csv file
#'
#' @param filepath a string indicating the relative or absolute
#'     filepath of the file to import
#'
#' @return a [tibble][tibble::tibble-package] of each row
#'   corrresponding to a line of the text file, with the column named
#'   "text"
import_csv <- function(filepath){
  readr::read_csv(filepath)
}
\end{minted}
\subsubsection{Import Excel}
\label{sec:org7535b78}
Unfortunately, much data exists in the Microsoft Excel format, but
this must be catered for. As tabular data, it is treated equivalently
to csv, with a wrapper around \mintinline{r}{readr::read_excel()}
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos]{r}
#' Import excel file
#'
#' @param filepath a string indicating the relative or absolute
#'     filepath of the file to import
#'
#' @return a [tibble][tibble::tibble-package] of each row corrresponding to a line of the text
#'     file, with the column named "text"
import_excel <- function(filepath){
  readxl::read_excel(filepath) ## %>%
    ## table_textcol()
}
\end{minted}
\subsubsection{Import Wrapper}
\label{sec:orgf966fad}
To have just one function required to import files, we define two
functions; one that imports any file, and one making use of it to
import multiple files.

The base wrapper function takes in the
filename, and other relevent information, handling the importation
process. It also stamps in the name of the document as a column.
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos]{r}
#' Base case for file import
#'
#' @param filepath string filepath of file for import
#'
#' @return imported file with document id
import_base_file <- function(filepath){
  filetype <- get_filetype(filepath)
  filename <- basename(filepath)
  if (filetype == "csv"){
    imported <- import_csv(filepath)
  } else if (filetype == "xlsx" | filetype == "xls") {
    imported <- import_excel(filepath)
  } else {
    imported <- import_txt(filepath)
  }
  imported %>%
    dplyr::mutate(doc_id = filename)
}
\end{minted}
The base file import is generalised to multiple files with a multiple
import function: this will be our sole import function
\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos]{r}
#' Import any number of files
#'
#' @param filepaths char vector of filepaths
#'
#' @return a [tibble][tibble::tibble-package] imported files with
#'   document id
#' 
#' @export
import_files <- function(filepaths){
  filepaths %>%
    purrr::map(import_base_file) %>%
    dplyr::bind_rows()
}
\end{minted}
\subsection{Formatting}
\label{sec:org9865b77}
From the imported files, we work at formatting them into a lossless
and efficient data structure that any analysis can make use of. Our
solution to this constraint is to separate and ID by each word in a
dataframe. To do this, we take the line ID, the sentence ID, then the
word ID, producing a dataframe that takes the following form:

\begin{center}
\begin{tabular}{rrrl}
line\textsubscript{id} & sentence\textsubscript{id} & word\textsubscript{id} & word\\
\hline
1 & 1 & 1 & the\\
1 & 1 & 2 & quick\\
2 & 1 & 3 & brown\\
\end{tabular}
\end{center}

The reason for the ID columns is the preservation of the structure of
the text; If required, the original text can be reconstructed in
entirety, sans minor punctuation differences. The following function
automatically formats any data of the format returned by the initial
import functions.

\begin{minted}[frame=lines,fontsize=\scriptsize,xleftmargin=\parindent,linenos]{r}
#' formats imported data into an analysis-ready format
#'
#' @param data a tibble formatted with a text and (optional) group
#'     column
#'
#' @return a [tibble][tibble::tibble-package] formatted such that columns correspond to
#'     identifiers of group, line, sentence, word (groups ignored)
#'
#' @export
format_data <- function(data){
  data %>%
    dplyr::mutate(line_id = dplyr::row_number()) %>% 
    tidytext::unnest_tokens(output = sentence, input = text, token = "sentences", to_lower = FALSE) %>%
    dplyr::mutate(sentence_id = dplyr::row_number()) %>%
    dplyr::group_by(sentence_id, add=TRUE) %>%
    dplyr::group_modify(~ {
      .x %>%
	tidytext::unnest_tokens(output = word, input = sentence, token = "words", to_lower=FALSE) %>%
	dplyr::mutate(word_id = dplyr::row_number())
    }) %>%
    ungroup_by("sentence_id")
}
\end{minted}
\subsection{Filtering}
\label{sec:orge4eec20}
Filtering of text is implemented directly with the
\mintinline{r}{dplyr::filter()} function, directly in the server of the shiny
app. Filtering can take place multiple times throughout an analysis.
The program is flexible enough such that after some initial analytics
have been done in the insight layer, preparation can be returned
to and the text can be filtered on based on the analytics.
\subsection{Lemmatisation}
\label{sec:org33549e9}
Lemmatisation is effectively the process of getting words into
dictionary form. It is a very complex, stochastic procedure, as
natural languages don't follow consistent and clear rules all the
time. Hence, models have to be used. Despite the burden, it is
generally worthwhile to lemmatise words for analytics, as there are
many cases of words not being considered significant, purely due to
taking so many different forms relative to others. Additionally,
stopwords work better when considering just the lemmatised form,
rather than attempting to exhaustively cover every possible form of a
word. \href{https://github.com/trinker/textstem/}{textstem} is an R
package allowing for easy lemmatisation, with it's function
\mintinline{r}{lemmatize_words()} transforming a vector of words into their
lemmatised forms (thus being compatible with \mintinline{r}{mutate()} straight
out of the box). We have the lemmatisation in this program managed
completely by this single function in the server end of the shiny app.
The package Udpipe was another option, but it requires downloading
model files, and performs far more in depth linguistic determinations
such as parts-of-speech tagging, that we don't need at this point.
Worth noting is that, like stopwords, there are different dictionaries
available for the lemmatisation process, but we will use the default,
as testing has shown it to be the simplest to set up and just as
reliable as the rest.
\subsection{Stemming}
\label{sec:orgca6bac4}
Stemming is far simpler than lemmatisation, being the removal of word
endings. This doesn't require as complex a model, as it is
deterministic. It is not quite as effective, as the base word ending
is not concatenated back on at the tail, so we are left with word
stumps and morphemes. However, it may sometimes be useful when the
lemmatisation model isn't working effectively, and textstem provides
the capability with \mintinline{r}{stem_words()}. We have not implemented this
yet, as it is not as essential to an analysis.
\section{Insight}
\label{sec:insight}

\section{Visualisation}
\label{sec:visualisation}

\section{User Interface}
\label{sec:user-interface}

\chapter{Conclusion}
\label{cha:conclusion}

\section{Summary}
\label{sec:summary}

\subsection{summarise successes}
\subsection{summarise failures}
\subsection{general thoughts on the topic}

\section{Recommendations}
\label{sec:recommendations}

\subsection{educational potential of text analytics}
\subsection{what else remains}

\chapter{Appendix}
\label{cha:appendix}

The following pages are a copy of the documentation for the R package
created as a part of this dissertation. They were automatically
generated through the Roxygen2 system.

\includepdf[pages=-]{img/inzightta_manual.pdf}

% \backmatter
\addcontentsline{toc}{chapter}{Glossary}
\printglossaries
\addcontentsline{toc}{chapter}{Index}
\printindex
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography
\end{document}

% \begin{listing}[ht]
% \inputminted[
% frame=lines,
% framesep=2mm,
% fontsize=\footnotesize,
% linenos
% ]{R}{src/table.R}
% \caption{Example Code}
% \label{lst:test}
% \end{listing}

% \begin{table}[ht]
%   \centering
%   \input{src/output_table}
%   \caption{test table}
%   \label{tab:test}
% \end{table}